### Automatic Evaluation for WebNLG Challenge 2017
Automatic scripts used to evaluate the WebNLG Challenge submissions.
_The WebNLG Challenge: Generating Text from RDF Data_. C. Gardent, A. Shimorina, S. Narayan, L. Perez-Beltrachini. Proceedings of INLG 2017. [pdf](http://webnlg.loria.fr/pages/webnlg-challenge-report.pdf)

Evaluation scripts calculate automatic scores for the whole submissions, for seen and unseen categories, for different categories (Monument, Artist, etc.) and for different sizes of triplesets (1-7). 

1. Run `python3 evaluation.py` to preprocess submission files and to generate reference files for automatic metric scorers.

    The files generated by that script are in `references/` and `teams/`.

2. Install scripts for METEOR, TER. See [guidelines provided on the baseline page](http://webnlg.loria.fr/pages/baseline.html).

3. Run `./bleu_eval_3ref.sh` to get BLEU scores.

    Scripts for METEOR and TER should be run from directories where the scorers are installed.

4. Run `./meteor_eval.sh` to get METEOR scores. You should modify the GLOBAL\_PATH variable. (METEOR scoring for all options may take around an hour.)
5. Run `./ter_eval.sh` to get TER scores. You should modify the GLOBAL\_PATH variable.

    The files generated by the scorers are in `eval/`.
